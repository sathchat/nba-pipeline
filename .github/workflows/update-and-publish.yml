name: Update NBA dataset and publish to Kaggle

on:
  schedule:
    - cron: "0 12 * * *"   # daily 12:00 UTC (~08:00 America/New_York)
  workflow_dispatch:
    inputs:
      smoketest:
        description: "Run a known in-season window and print counts (does not publish empties)"
        required: false
        default: "false"
      start:
        description: "Optional START_DATE (YYYY-MM-DD) for manual run"
        required: false
        default: ""
      end:
        description: "Optional END_DATE (YYYY-MM-DD) for manual run"
        required: false
        default: ""

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Increased for potential Kaggle delays

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps
        run: |
          set -euxo pipefail
          python -m pip install -U pip
          pip install -r requirements.txt
          pip install kaggle

      - name: Seed empty CSVs if missing (headers only)
        run: |
          set -euxo pipefail
          python - <<'PY'
          import os, csv
          os.makedirs('export', exist_ok=True)
          def ensure(path, cols):
              if not os.path.exists(path):
                  with open(path,'w',newline='',encoding='utf-8') as f:
                      csv.writer(f).writerow(cols)
          games_cols = ["gameId","gameCode","gameDateEt","gameStatusText","period","gameClock","arenaName",
                        "homeTeamId","homeTeamTricode","homeScore","awayTeamId","awayTeamTricode","awayScore"]
          players_cols = ["gameId","playerId","teamId","teamTricode","firstName","familyName","jerseyNum","position",
                          "minutes","points","reboundsTotal","assists","steals","blocks","turnovers",
                          "fieldGoalsMade","fieldGoalsAttempted","threePointersMade","threePointersAttempted",
                          "freeThrowsMade","freeThrowsAttempted","plusMinus","didNotPlay","notPlayingReason"]
          team_cols = ["gameId","teamId","teamTricode","teamCity","teamName","opponentTeamId","home","win",
                       "points","reboundsTotal","assists","steals","blocks","turnovers",
                       "fieldGoalsMade","fieldGoalsAttempted","threePointersMade","threePointersAttempted",
                       "freeThrowsMade","freeThrowsAttempted"]
          ensure('export/games.csv', games_cols)
          ensure('export/player_stats.csv', players_cols)
          ensure('export/team_stats.csv', team_cols)
          PY

      # Optional smoketest for manual runs (prints counts and heads)
      - name: Smoketest (known in-season window)
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.smoketest == 'true' }}
        run: |
          set -euxo pipefail
          START_DATE="${{ inputs.start != '' && inputs.start || '2016-01-15' }}"
          END_DATE="${{ inputs.end != '' && inputs.end || '2016-01-16' }}"
          START_DATE="$START_DATE" END_DATE="$END_DATE" python src/ingest.py
          echo "Line counts (should be >1 each):"
          wc -l export/games.csv export/team_stats.csv export/player_stats.csv || true
          echo "Heads:"
          head -n 5 export/games.csv || true
          head -n 5 export/team_stats.csv || true
          head -n 5 export/player_stats.csv || true

      - name: Ingest latest NBA data (daily path)
        if: ${{ ! (github.event_name == 'workflow_dispatch' && inputs.smoketest == 'true') }}
        run: |
          set -euxo pipefail
          if [ -n "${{ inputs.start }}" ] && [ -n "${{ inputs.end }}" ]; then
            START_DATE="${{ inputs.start }}" END_DATE="${{ inputs.end }}" python src/ingest.py || {
              echo "‚ùå Ingestion failed with custom dates"
              exit 1
            }
          else
            python src/ingest.py || {
              echo "‚ùå Ingestion failed with default dates"
              exit 1
            }
          fi
          echo "Heads:"
          head -n 5 export/games.csv || true
          head -n 5 export/team_stats.csv || true
          head -n 5 export/player_stats.csv || true

      - name: Check data counts (decide whether to publish)
        id: check
        run: |
          set -euxo pipefail
          gc=$( [ -f export/games.csv ] && wc -l < export/games.csv || echo 0 )
          tc=$( [ -f export/team_stats.csv ] && wc -l < export/team_stats.csv || echo 0 )
          pc=$( [ -f export/player_stats.csv ] && wc -l < export/player_stats.csv || echo 0 )
          echo "games=$gc team=$tc players=$pc"
          if [ "$gc" -gt 1 ] && [ "$tc" -gt 1 ] && [ "$pc" -gt 1 ]; then
            echo "has_data=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_data=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Validate dataset sizes
        if: steps.check.outputs.has_data == 'true'
        run: |
          set -euxo pipefail
          gc=$(wc -l < export/games.csv)
          tc=$(wc -l < export/team_stats.csv)
          pc=$(wc -l < export/player_stats.csv)
          
          if [ "$gc" -lt 2 ] || [ "$tc" -lt 4 ] || [ "$pc" -lt 20 ]; then
            echo "‚ùå Dataset sizes seem suspiciously small"
            echo "Games: $gc, Teams: $tc, Players: $pc"
            exit 1
          fi

      - name: Configure Kaggle CLI
        if: steps.check.outputs.has_data == 'true'
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          set -euxo pipefail
          mkdir -p ~/.kaggle
          printf '{"username":"%s","key":"%s"}\n' "$KAGGLE_USERNAME" "$KAGGLE_KEY" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json
          kaggle --version
          echo "Using Kaggle owner: $KAGGLE_USERNAME"

      - name: Patch dataset owner in metadata
        if: steps.check.outputs.has_data == 'true'
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
        run: |
          set -euxo pipefail
          sed -i -E "s/\"id\": \"[^\"]*\\/nba-daily-updates\"/\"id\": \"${KAGGLE_USERNAME//\//\\/}\\/nba-daily-updates\"/g" dataset-metadata.json
          cat dataset-metadata.json

      - name: Update dataset metadata with timestamp
        if: steps.check.outputs.has_data == 'true'
        run: |
          set -euxo pipefail
          CURRENT_DATE=$(date -u +"%Y-%m-%d")
          sed -i "s/\"title\": \".*\"/\"title\": \"NBA Daily Updates - $CURRENT_DATE\"/g" dataset-metadata.json
          cat dataset-metadata.json

      - name: Create or version Kaggle dataset (UPLOAD RAW FILES)
        if: steps.check.outputs.has_data == 'true'
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
        run: |
          set -euxo pipefail
          DS="${KAGGLE_USERNAME}/nba-daily-updates"
          # Do NOT use -r zip or --dir-mode zip. Let Kaggle ingest raw CSVs.
          if kaggle datasets status "$DS" > /dev/null 2>&1; then
            kaggle datasets version -p . -m "Daily update"
          else
            kaggle datasets create -p . --public
          fi
          echo "Dataset URL: https://www.kaggle.com/datasets/$KAGGLE_USERNAME/nba-daily-updates"
          echo "Files now on Kaggle:"
          kaggle datasets files "$DS" || true

      - name: Verify Kaggle upload
        if: steps.check.outputs.has_data == 'true'
        run: |
          set -euxo pipefail
          DS="${KAGGLE_USERNAME}/nba-daily-updates"
          echo "Waiting 30 seconds for Kaggle processing..."
          sleep 30
          kaggle datasets status "$DS" || {
            echo "‚ùå Failed to verify dataset status on Kaggle"
            exit 1
          }
          echo "‚úÖ Dataset successfully published to Kaggle"

      - name: Explain skip
        if: steps.check.outputs.has_data != 'true'
        run: echo "Skip publish: no data collected (off-season window, API outage, or wrong dates)."

  notify:
    needs: run
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Notify on failure
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'üö® NBA dataset update failed! Check the workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}'
            })
